AstroGPT: A GenAI Model for Simplifying Astrophysics Research
-------------------------------------------------------------

AstroGPT is a GenAI system fine-tuned from GPT-2 that rewrites complex astrophysics research abstracts—sourced directly from arXiv—into clear, accessible explanations for the general public. The pipeline includes automated data ingestion, teacher prompting, dataset creation, and model fine-tuning.

-------------------------------------------------------------
FEATURES
- Automated arXiv ingestion for astro-ph abstracts
- Teacher-prompt generation for high-quality explanations
- GPT-2 fine-tuning pipeline built entirely in Python
- Clean class-based architecture (data, prompts, model, inference)
- Simple API for generating layperson-friendly explanations
- Easily extendable for larger models or RLHF in future versions

-------------------------------------------------------------
PROJECT STRUCTURE

AstroGPT/
  data/                      - Raw abstracts and processed JSONL datasets
  notebooks/                 - Exploration and development notebooks
  src/
    data_loader.py           - arXiv ingestion utilities
    teacher_prompts.py       - Teacher prompt generation logic
    model.py                 - GPT-2 fine-tuning class
    inference.py             - Inference pipeline
  scripts/
    build_dataset.py         - Automated dataset construction
    train.py                 - Training script
    explain.py               - CLI tool for running AstroGPT
  README.txt

-------------------------------------------------------------
INSTALLATION
.. Coming Soon ..

-------------------------------------------------------------
USAGE

.. Coming Soon ..

-------------------------------------------------------------
AUTHOR
Pushpita Das
Astrophysicist | ML Researcher | GenAI Developer


-------------------------------------------------------------
DETAILED PROJECT STRUCTURE
src/
  miniastrolm/
    data/
      arxiv_fetcher.py        # (1) ingest
      db.py                   # sqlite connect + schema helpers (2)
      dataset_builder.py      # (3,5) batches + jsonl creation

    llm/
      teacher.py              # (4) load teacher + generate_text()
      student.py              # (6) load student / inference
      prompts.py              # teacher prompt templates + style rules

    training/
      finetune_gpt2.py        # (6) training entrypoint
      collate.py              # tokenization / formatting

    eval/
      eval_rules.py           # (7) banned phrases, structure checks
      sample_compare.py       # before/after comparisons

data/
  raw/                        # downloaded metadata / dumps
  processed/
    mini_astrolm.db           # sqlite DB (2)
    batches/                  # batch_### files (3)
    teacher_outputs/          # teacher JSON outputs (4)
    train.jsonl               # final training set (5)
outputs/
  checkpoints/                # student weights (6)
  reports/                    # eval outputs (7)
notebooks/
  debug_*.ipynb               # your experiments / debugging