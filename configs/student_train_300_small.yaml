model:
  model_name: "gpt2-medium"
  max_length: 768
  use_lora: true
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.01
  

data:
  train_path: "/Users/pushpita/Documents/ML Projects/Building_LLM_from_scratch/MiniAstroLM/data/v1_judged_samples/fit_300.jsonl"
  val_path: "/Users/pushpita/Documents/ML Projects/Building_LLM_from_scratch/MiniAstroLM/data/v1_judged_samples/validation.jsonl"
  val_max_samples: 64
  max_samples: 300

training:
  batch_size: 1
  lr: 5.e-5
  max_steps: 600
  weight_decay: 0.0
  seed: 42
  device: "mps"
  freeze_embeddings: false
  n_freeze_blocks: 0
  gradient_accumulation_steps: 8
  use_grad_accum_loss: true
  scheduler: "constant"
  warmup_ratio: 0.0

output:
  output_dir: "/Users/pushpita/Documents/ML Projects/Building_LLM_from_scratch/MiniAstroLM/data/student/train_300_medium"
